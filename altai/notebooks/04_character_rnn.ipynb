{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# load up our text\n",
    "text = open('../data/input_text.txt', 'r').read()\n",
    "\n",
    "# extract all (unique) characters\n",
    "# these are our \"categories\" or \"labels\"\n",
    "chars = list(set(text))\n",
    "\n",
    "# set a fixed vector size\n",
    "# so we look at specific windows of characters\n",
    "max_len = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll define our RNN. Keras makes this trivial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(512, return_sequences=True, input_shape=(max_len, len(chars))))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(512, return_sequences=False))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(len(chars)))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're framing our task as a classification task. Given a sequence of characters, we want to predict the next character. We equate each character with some label or category (e.g. \"a\" is 0, \"b\" is 1, etc).\n",
    "\n",
    "We use the softmax activation function on our output layer - this function is used for categorical output. It turns the output into a probability distribution over the categories (i.e. it makes the values the network outputs sum to 1). So the network will essentially tell us how strongly it feels about each character being the next one.\n",
    "\n",
    "The categorical cross-entropy loss the standard loss function for multilabel classification.\n",
    "\n",
    "We use dropout here to prevent overfitting - we don't want the network to just return things already in the text, we want it to have some wiggle room and create novelty! Dropout is a technique where, in training, some percent (here, 20%) of random neurons of the associated layer are \"turned off\" for that epoch. This prevents overfitting but preventing the network from relying on particular neurons.\n",
    "\n",
    "That's it for the network architecture!\n",
    "\n",
    "To train, we have to do some additional preparation. We need to chop up the text into character sequences of the length we specified (`max_len`) - these are our training inputs. We match them with the character that immediately follows each sequence. These are our expected training outputs.\n",
    "\n",
    "For example, say we have the following text (this quote is from Zhuang Zi). With `max_len=20`, we could manually create the first couple training examples like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "example = \"The fish trap exists because of the fish. Once you've gotten the fish you can forget the trap. The rabbit snare exists because of the rabbit. Once you've gotten the rabbit, you can forget the snare. Words exist because of meaning. Once you've gotten the meaning, you can forget the words. Where can I find a man who has forgotten words so that I may have a word with him?\"\n",
    "\n",
    "# step size here is 3, but we can vary that\n",
    "input_1 = example[0:20]\n",
    "true_output_1 = example[20]\n",
    "# >>> 'The fish trap exists'\n",
    "# >>> ' '\n",
    "\n",
    "input_2 = example[3:23]\n",
    "true_output_2 = example[23]\n",
    "# >>> 'fish trap exists be'\n",
    "# >>> 'c'\n",
    "\n",
    "input_3 = example[6:26]\n",
    "true_output_3 = example[26]\n",
    "# >>> 'sh trap exists becau'\n",
    "# >>> 's'\n",
    "\n",
    "# etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "readily iden\n",
      "t\n"
     ]
    }
   ],
   "source": [
    "# We can generalize this like so:\n",
    "step = 3\n",
    "inputs = []\n",
    "outputs = []\n",
    "for i in range(0, len(text) - max_len, step):\n",
    "    inputs.append(text[i:i+max_len])\n",
    "    outputs.append(text[max_len])\n",
    "    \n",
    "print(inputs[0])\n",
    "print(outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'m': 0, 'b': 20, 'd': 1, '0': 2, 'v': 32, 'n': 3, 'a': 29, '4': 4, 'p': 5, '8': 7, 'e': 8, 's': 9, 'c': 10, '/': 11, '-': 12, '\\n': 13, 'h': 14, '7': 15, '5': 16, '2': 17, '1': 18, 'k': 6, \"'\": 21, 'x': 22, 'f': 23, 'q': 24, 'z': 25, '3': 26, 'j': 27, 't': 28, 'o': 31, 'l': 30, '\"': 34, 'g': 33, '(': 36, 'i': 35, 'w': 19, '9': 40, ')': 38, ',': 39, 'u': 41, 'y': 42, ' ': 37, 'r': 43}\n"
     ]
    }
   ],
   "source": [
    "# We also need to map each character to a label and create a reverse mapping to use later:\n",
    "char_labels = {ch:i for i, ch in enumerate(chars)}\n",
    "labels_char = {i:ch for i, ch in enumerate(chars)}\n",
    "\n",
    "# save these for later\n",
    "json.dump(char_labels, open('../data/char_labels.json', 'w'))\n",
    "json.dump(labels_char, open('../data/labels_char.json', 'w'))\n",
    "\n",
    "print(char_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can start constructing our numerical input 3-tensor and output matrix. Each input example (i.e. a sequence of characters) is turned into a matrix of one-hot vectors; that is, a bunch of vectors where the index corresponding to the character is set to 1 and all the rest are set to zero.\n",
    "\n",
    "For example, if we have the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 0, 1, 0, 0],\n",
       " [1, 0, 0, 0, 0],\n",
       " [0, 1, 0, 0, 0],\n",
       " [0, 0, 0, 0, 1],\n",
       " [0, 0, 0, 1, 0],\n",
       " [1, 0, 0, 0, 0],\n",
       " [0, 1, 0, 0, 0]]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# assuming max_len = 7\n",
    "# so our examples have 7 characters\n",
    "example = 'cab dab'\n",
    "example_char_labels = {\n",
    "    'a': 0,\n",
    "    'b': 1,\n",
    "    'c': 2,\n",
    "    'd': 3,\n",
    "    ' ' : 4\n",
    "}\n",
    "\n",
    "# matrix form\n",
    "# only five characters, so the vectors only need to have five components\n",
    "[\n",
    "    [0, 0, 1, 0, 0], # c\n",
    "    [1, 0, 0, 0, 0], # a\n",
    "    [0, 1, 0, 0, 0], # b\n",
    "    [0, 0, 0, 0, 1], # (space)\n",
    "    [0, 0, 0, 1, 0], # d\n",
    "    [1, 0, 0, 0, 0], # a\n",
    "    [0, 1, 0, 0, 0]  # b\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That matrix represents a single training example, so we have a stack of those matrices (hence a 3-tensor). The outputs for each example are each a one-hot vector. With that in mind:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# using bool to reduce memory usage\n",
    "X = np.zeros((len(inputs), max_len, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(inputs), len(chars)), dtype=np.bool)\n",
    "\n",
    "# set the appropriate indices to 1 in each one-hot vector\n",
    "for i, example in enumerate(inputs):\n",
    "    for t, char in enumerate(example):\n",
    "        X[i, t, char_labels[char]] = 1\n",
    "    y[i, char_labels[char]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-3c0789764f27>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mtext_to_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'abc'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-10-3c0789764f27>\u001b[0m in \u001b[0;36mtext_to_matrix\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtext_to_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[1;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchars\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbool\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchar\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchar_labels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mchar\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def text_to_matrix(text):\n",
    "    assert len(text) == max_len\n",
    "    X = np.zeros((1, max_len, len(chars)), dtype=np.bool)\n",
    "    for i, char in enumerate(text):\n",
    "        X[0, i, char_labels[char]] = 1\n",
    "    return X   \n",
    "    \n",
    "text_to_matrix('abc').astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our training data, we can start training. Keras also makes this easy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# more epochs is usually better, but training can be very slow if not on a GPU\n",
    "epochs = 10\n",
    "model.fit(X, y, batch_size=128, nb_epoch=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's much more fun to see your network's ramblings as it's training, so let's write a function to produce text from the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate(temperature=0.35, seed=None, predicate=lambda x: len(x) < 100):\n",
    "    if len(seed) < max_len:\n",
    "        raise Exception('Seed text must be at least {} chars long'.format(max_len))\n",
    "\n",
    "    # if no seed text is specified, randomly select a chunk of text\n",
    "    if seed is None:\n",
    "        start_idx = random.randint(0, len(text) - max_len - 1)\n",
    "        seed = text[start_idx:start_idx + max_len]\n",
    "\n",
    "    sentence = seed\n",
    "    generated = sentence\n",
    "\n",
    "    while predicate(generated):\n",
    "        # generate the input tensor\n",
    "        # from the last max_len characters generated so far\n",
    "        x = np.zeros((1, max_len, len(chars)))\n",
    "        for t, char âˆˆ enumerate(sentence):\n",
    "            x[0, t, char_labels[char]] = 1.\n",
    "\n",
    "        # this produces a probability distribution over characters\n",
    "        probs = model.predict(X, verbose=0)[0]\n",
    "\n",
    "        # sample the character to use based on the predicted probabilities\n",
    "        next_idx = sample(probs, temperature)\n",
    "        next_char = labels_char[next_idx]\n",
    "\n",
    "        generated += next_char\n",
    "        sentence = sentence[1:] + next_char\n",
    "    return generated\n",
    "\n",
    "def sample(probs, temperature):\n",
    "    \"\"\"samples an index from a vector of probabilities\"\"\"\n",
    "    a = np.log(a)/temperature\n",
    "    a = np.exp(a)/np.sum(np.exp(a))\n",
    "    return np.argmax(np.random.multinomial(1, a, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The temperature controls how random we want the network to be. Lower temperatures favors more likely values, whereas higher temperatures introduce more and more randomness. At a high enough temperature, values will be chosen at random.\n",
    "\n",
    "With this generation function we can modify how we train the network so that we see some output at each step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "for i in range(epochs):\n",
    "    print('epoch', i)\n",
    "\n",
    "    # set nb_epoch to 1 since we're iterating manually\n",
    "    model.fit(X, y, batch_size=128, nb_epoch=1)\n",
    "\n",
    "    # preview\n",
    "    for temp in [0.2, 0.5, 1., 1.2]:\n",
    "        print('\\n\\ttemperature:', temp)\n",
    "        print(generate(temperature=temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
