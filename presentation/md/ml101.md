# ML101

Francis Tseng (@frnsys)

---

let's start setting up b/c it will probably take awhile â³

    git clone https://github.com/frnsys/ml101
    cd ml101
    pip install -r requirements.txt
    jupyter notebook

---

## what we'll cover today

how to use:

1. [`scikit-learn`](http://scikit-learn.org/stable/) for supervised linear learning
2. [`keras`](http://keras.io/) for neural networks
3. [`pandas`](http://pandas.pydata.org/) for handling data
4. [`matplotlib`](http://matplotlib.org/) and [`seaborn`](https://web.stanford.edu/~mwaskom/software/seaborn/) for visualizing data

---

## assumptions

- you have some python ğŸ experience
- you know a bit of high school ğŸ« math

---

## What do you know about machine learning? ğŸ’¬

---

ok, so what the heck is machine learning doing? what is it even for?

## ğŸ’»ğŸ¤”

---

## âœ¨~modeling the world~âœ¨

---

we can model phenomena, both natural and artificial, as mathematical functions

# y âŸ¶ $f(x)$
# ğŸ’« âŸ¶ $f(âš›)$
# ğŸ’¹ âŸ¶ $f(ğŸ“°)$
# ğŸ¼ âŸ¶ $f(ğŸ¼)$
# ğŸ¼ âŸ¶ $f(ğŸ¼,ğŸ˜¶)$

---

how could we come up with such a function? ğŸ¤”

there are _infinitely_ many of them

---

we could go and observe a lot of things and then try to figure out some equation that matched what we observed.

# ğŸŒğŸ‘€

---

but that is a tedious process...

---

ok, let's have computers do it for us then

# ğŸ¤–

---

this is in essence what "machine learning" is:

### _using computers to learn functions from observations (data)_

---

once you learn your mystery function, there's so much you can do with it:

- predict things
- automate things/make decisions
- gain insight into a system
- emulate a system

---

let's get more concrete

# ğŸ„

---

let's say you're a farmer. you have a herd of cows.

ğŸ„ğŸ„ğŸ„ğŸ„

you want to estimate how heavy a cow will grow to be given its birth weight.

---

you collect some data which looks like this:

![](assets/ml101/cow_data.png)

---

we can try to learn a function that _fits_ this data; i.e. that best describes (models) the relationship between the birth weight and mature weight.

---

to better appreciate all the help computers give us, let's try this manually first.

---

![](assets/ml101/cow_data.png)

this data looks like a line doesn't it?

---

remember that a line can be described in the general form of

$$
y = mx + b
$$

---

remember that lines vary depending on what the values of $m$ and $b$ are:

![Lines](assets/ml101/lines.svg)

we say that $m$ and $b$ _parameterize_ the function ($m$ and $b$ are called "parameters").

---

these parameters define a unique function, and thus when we "learn" a particular function, we are actually learning these parameters!

---

if we wanted to learn these parameters manually, we could use the good old "guess-and-check" method:

![](assets/ml101/cow_guess.png)

---

this was an easy dataset - real world data may be much more convoluted, not describable by a line, in many more dimensions, etc...manually figuring out the function gets kinda hard then.

![](assets/ml101/nonconvex.svg)

---

(example in notebook ğŸ—’)

---

essentially every machine learning technique learns what these parameters are, and one of what differentiates algorithms is the approach with which they do this learning.

another main differentiator is what _kinds_ of functions the algorithm can learn - some can only learn lines, while others can learn much fancier functions.

---

## so how the heck does the algorithm learn the parameters?

# ğŸ¤”

---

it varies, but usually via a _cost_ or _objective_ function (often notated $J$).

this tells the algorithm how "wrong" it is with its current guesses for the parameters on the training data.

---

the algorithm iteratively tries different parameters (i.e. different guesses at the underlying function) until it can minimize this error.

that is, it tries to _optimize_ the parameters for the cost function.

---

different optimization algorithms have different ways of picking new guesses.

the most popular one is _gradient descent_, which looks for the direction in which the error is decreasing, and then takes a step in that direction.

# ğŸ—»

---

if we were just finding $m$, this might look like:

![](assets/ml101/gradient_descent.svg)

---

if we're finding both $m$ and $b$, this might look like:

![](assets/ml101/gradient_descent_3d.svg)

---

there are other optimization methods such as genetic algorithms, particle swarm optimization, etc...

but gradient descent is by far the most common.

---

another example:

# the office of social health

![](assets/ml101/safety_in_numbers.sm.jpg)

(based on the psycho-pass series)

---

## ok, let's switch gears and talk about ğŸ‰~neural networks~ğŸ‰

they have many, many fun and interesting applications

---

a neural network does essentially what we were doing before, but it can learn more sophisticated functions ğŸ’ªğŸ½

---

![](assets/ml101/nn.png)
###### (from [neuralnetworksanddeeplearning.com](http://neuralnetworksanddeeplearning.com/))

---

one particularly fun kind of neural network is a _recurrent neural network_

![](assets/ml101/rnn.png)
###### (from [hexahedria](http://www.hexahedria.com/2015/08/03/composing-music-with-recurrent-neural-networks/))

these are great for modeling _sequences_, e.g. textğŸ“, musicğŸ¶, time seriesğŸ•, and so on

---

there are examples abound here, such as generated texts in the style of [Finnegan's Wake](https://www.countbayesie.com/blog/2015/5/24/writing-finnegans-wake-with-a-recurrent-neural-net), endless new episode scripts for [Full House](http://fullest.house/), and algorithimically-composed [sheet music](https://medium.com/dbrs-innovation-labs/in-his-novel-galatea-2-2-e9d11c9b7c2a#.xd00cremz).

---

sounds like a lot of fun, but there's one hangup...

# ğŸ™

---

## what if the inputs we want to use aren't numbers? ğŸ¤”

for example, how do you put a piece of text into a function?

# ğŸ“– âŸ¶ ğŸ”¢

how do you _represent_ it as a number?

---

this is the problem of _representation_, and it is key to machine learning.

coming up with good representations is called _feature engineering_ and it can be more of an art than a skill.

# ğŸ›  + ğŸ¨

---

for text, one option is to map each word to a unique number.

# ğŸ™ƒ

---

(example in notebook ğŸ—’)

---

one last kind of machine learning that I really enjoy:

## reinforcement learning

# ğŸ¶

---

what we've seen so far is pretty cool, but things _really_ get fun when we start designing agents that can act independently;

where they can interact with an environment on their own, learn from it, and develop new behaviors ğŸ¤–

---

reinforcement learning is behind neat stuff like Google DeepMind's _AlphaGo_ ğŸ† and their [Atari-playing AIs](https://www.youtube.com/watch?v=Vr5MR5lKOc8) ğŸ•¹

---

the behavior of the agent can be described by (you guessed it) a function, and of course we want to learn it ğŸ“

---

the basic idea:

- we model the world as various _states_ ğŸ™ğŸ½ the agent can be in.
- the agent can take _actions_ ğŸ§ that move between these states.
- each state has an associated _reward_ ğŸ’° (or punishment ğŸ—¡).
- the agent _explores_ ğŸ—º these states and learns which sequence of actions tend to lead to more rewards.

this is given to us by a function, usually called $q$, which, given a state, maps actions to values - so this is the function we want to learn.

---

![](assets/ml101/mdp.png)

a very simple set of agent states and actions. this agent will eventually spend all of its time sleeping ğŸ’¤.

---

a very simple environment for a RL agent is a "grid world"

![](assets/ml101/mdp_grid.png)

---

![](assets/ml101/workshop_rl.gif)

(example in notebook ğŸ—’)

---

through this process you've seen how much decision making we as people had to make with regards to what goes into the algorithm, which one we use, and so on.

# ğŸ¤”

hopefully it is clear that machine learning can't be claimed to be "fully objective" ğŸš½ or anything of the sort

---

thanks!

if you want to go deeper: [frnsys.com/ai_notes/](http://frnsys.com/ai_notes/)

~ @frnsys